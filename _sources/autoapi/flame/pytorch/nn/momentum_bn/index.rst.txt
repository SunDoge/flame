:py:mod:`flame.pytorch.nn.momentum_bn`
======================================

.. py:module:: flame.pytorch.nn.momentum_bn

.. autoapi-nested-parse::

   https://github.com/zengarden/momentum2-teacher



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   flame.pytorch.nn.momentum_bn.MomentumBatchNorm2d
   flame.pytorch.nn.momentum_bn.MomentumBatchNorm1d




.. py:class:: MomentumBatchNorm2d(num_features, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True, total_iters=100)

   Bases: :py:obj:`torch.nn.BatchNorm2d`

   Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs
   with additional channel dimension) as described in the paper
   `Batch Normalization: Accelerating Deep Network Training by Reducing
   Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .

   .. math::

       y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

   The mean and standard-deviation are calculated per-dimension over
   the mini-batches and :math:`\gamma` and :math:`\beta` are learnable parameter vectors
   of size `C` (where `C` is the input size). By default, the elements of :math:`\gamma` are set
   to 1 and the elements of :math:`\beta` are set to 0. The standard-deviation is calculated
   via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.

   Also by default, during training this layer keeps running estimates of its
   computed mean and variance, which are then used for normalization during
   evaluation. The running estimates are kept with a default :attr:`momentum`
   of 0.1.

   If :attr:`track_running_stats` is set to ``False``, this layer then does not
   keep running estimates, and batch statistics are instead used during
   evaluation time as well.

   .. note::
       This :attr:`momentum` argument is different from one used in optimizer
       classes and the conventional notion of momentum. Mathematically, the
       update rule for running statistics here is
       :math:`\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t`,
       where :math:`\hat{x}` is the estimated statistic and :math:`x_t` is the
       new observed value.

   Because the Batch Normalization is done over the `C` dimension, computing statistics
   on `(N, H, W)` slices, it's common terminology to call this Spatial Batch Normalization.

   :param num_features: :math:`C` from an expected input of size
                        :math:`(N, C, H, W)`
   :param eps: a value added to the denominator for numerical stability.
               Default: 1e-5
   :param momentum: the value used for the running_mean and running_var
                    computation. Can be set to ``None`` for cumulative moving average
                    (i.e. simple average). Default: 0.1
   :param affine: a boolean value that when set to ``True``, this module has
                  learnable affine parameters. Default: ``True``
   :param track_running_stats: a boolean value that when set to ``True``, this
                               module tracks the running mean and variance, and when set to ``False``,
                               this module does not track such statistics, and initializes statistics
                               buffers :attr:`running_mean` and :attr:`running_var` as ``None``.
                               When these buffers are ``None``, this module always uses batch statistics.
                               in both training and eval modes. Default: ``True``

   Shape:
       - Input: :math:`(N, C, H, W)`
       - Output: :math:`(N, C, H, W)` (same shape as input)

   Examples::

       >>> # With Learnable Parameters
       >>> m = nn.BatchNorm2d(100)
       >>> # Without Learnable Parameters
       >>> m = nn.BatchNorm2d(100, affine=False)
       >>> input = torch.randn(20, 100, 35, 45)
       >>> output = m(input)

   .. py:method:: momentum_cosine_decay()


   .. py:method:: forward(x)



.. py:class:: MomentumBatchNorm1d(num_features, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True, total_iters=100)

   Bases: :py:obj:`torch.nn.BatchNorm1d`

   Applies Batch Normalization over a 2D or 3D input as described in the paper
   `Batch Normalization: Accelerating Deep Network Training by Reducing
   Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .

   .. math::

       y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

   The mean and standard-deviation are calculated per-dimension over
   the mini-batches and :math:`\gamma` and :math:`\beta` are learnable parameter vectors
   of size `C` (where `C` is the number of features or channels of the input). By default, the
   elements of :math:`\gamma` are set to 1 and the elements of :math:`\beta` are set to 0. The
   standard-deviation is calculated via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.

   Also by default, during training this layer keeps running estimates of its
   computed mean and variance, which are then used for normalization during
   evaluation. The running estimates are kept with a default :attr:`momentum`
   of 0.1.

   If :attr:`track_running_stats` is set to ``False``, this layer then does not
   keep running estimates, and batch statistics are instead used during
   evaluation time as well.

   .. note::
       This :attr:`momentum` argument is different from one used in optimizer
       classes and the conventional notion of momentum. Mathematically, the
       update rule for running statistics here is
       :math:`\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t`,
       where :math:`\hat{x}` is the estimated statistic and :math:`x_t` is the
       new observed value.

   Because the Batch Normalization is done over the `C` dimension, computing statistics
   on `(N, L)` slices, it's common terminology to call this Temporal Batch Normalization.

   :param num_features: number of features or channels :math:`C` of the input
   :param eps: a value added to the denominator for numerical stability.
               Default: 1e-5
   :param momentum: the value used for the running_mean and running_var
                    computation. Can be set to ``None`` for cumulative moving average
                    (i.e. simple average). Default: 0.1
   :param affine: a boolean value that when set to ``True``, this module has
                  learnable affine parameters. Default: ``True``
   :param track_running_stats: a boolean value that when set to ``True``, this
                               module tracks the running mean and variance, and when set to ``False``,
                               this module does not track such statistics, and initializes statistics
                               buffers :attr:`running_mean` and :attr:`running_var` as ``None``.
                               When these buffers are ``None``, this module always uses batch statistics.
                               in both training and eval modes. Default: ``True``

   Shape:
       - Input: :math:`(N, C)` or :math:`(N, C, L)`, where :math:`N` is the batch size,
         :math:`C` is the number of features or channels, and :math:`L` is the sequence length
       - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)

   Examples::

       >>> # With Learnable Parameters
       >>> m = nn.BatchNorm1d(100)
       >>> # Without Learnable Parameters
       >>> m = nn.BatchNorm1d(100, affine=False)
       >>> input = torch.randn(20, 100)
       >>> output = m(input)

   .. py:method:: momentum_cosine_decay()


   .. py:method:: forward(x)



