:py:mod:`flame.pytorch.distributed_training`
============================================

.. py:module:: flame.pytorch.distributed_training


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   flame.pytorch.distributed_training.DistOptions



Functions
~~~~~~~~~

.. autoapisummary::

   flame.pytorch.distributed_training.get_dist_options
   flame.pytorch.distributed_training._init_process_group_fn
   flame.pytorch.distributed_training.start_distributed_training
   flame.pytorch.distributed_training.get_available_local_dist_url
   flame.pytorch.distributed_training.init_cpu_process_group
   flame.pytorch.distributed_training.start_training



Attributes
~~~~~~~~~~

.. autoapisummary::

   flame.pytorch.distributed_training._logger


.. py:data:: _logger
   

   

.. py:class:: DistOptions

   Bases: :py:obj:`flame.argument.BasicArgs`

   .. py:attribute:: rank_start
      :annotation: :int

      

   .. py:attribute:: world_size
      :annotation: :Optional[int]

      

   .. py:attribute:: dist_backend
      :annotation: :str

      

   .. py:attribute:: dist_host
      :annotation: :str

      

   .. py:attribute:: dist_port
      :annotation: :Optional[int]

      

   .. py:attribute:: nprocs
      :annotation: :Optional[int]

      

   .. py:method:: dist_url(self)
      :property:


   .. py:method:: get_rank(self, local_rank)


   .. py:method:: find_free_port(self)



.. py:function:: get_dist_options()


.. py:function:: _init_process_group_fn(local_rank, worker_fn, dist_options, *args)

   wrapper function for worker_fn

   必须定义成可以被pickle的函数。

   1. compute rank
   2. init_process_group
   3. set cuda device if cuda is available
   4. call worker_fn



.. py:function:: start_distributed_training(worker_fn, dist_options, args = ())

   helper function for distributed training

   1. get number of GPUs
   2. start N process, N = number of GPUs
   3. init_process_group
   4. call worker_fn with args



.. py:function:: get_available_local_dist_url()

   helper function for single node distributed training

   Get a local dist url like::

       tcp://127.0.0.1:12345




.. py:function:: init_cpu_process_group(rank = 0, world_size = 1, dist_url = 'tcp://127.0.0.1:12345')

   helper function for testing distributed training

   测试函数，方便在CPU环境下测试分布式操作，比如测试MoCo的shuffle bn。
   不要在生产环境中使用。



.. py:function:: start_training(worker_fn, args = ())


