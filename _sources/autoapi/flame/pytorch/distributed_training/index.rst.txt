:mod:`flame.pytorch.distributed_training`
=========================================

.. py:module:: flame.pytorch.distributed_training


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   flame.pytorch.distributed_training.DistOptions



Functions
~~~~~~~~~

.. autoapisummary::

   flame.pytorch.distributed_training.get_dist_options
   flame.pytorch.distributed_training._init_process_group_fn
   flame.pytorch.distributed_training.start_distributed_training
   flame.pytorch.distributed_training.get_available_local_dist_url
   flame.pytorch.distributed_training.init_cpu_process_group
   flame.pytorch.distributed_training.start_training



Attributes
~~~~~~~~~~

.. autoapisummary::

   flame.pytorch.distributed_training._logger


.. data:: _logger
   

   

.. class:: DistOptions

   Bases: :py:obj:`typed_args.TypedArgs`

   .. attribute:: dist
      :annotation: :bool

      

   .. attribute:: rank_start
      :annotation: :int

      

   .. attribute:: world_size
      :annotation: :Optional[int]

      

   .. attribute:: dist_backend
      :annotation: :str

      

   .. attribute:: dist_host
      :annotation: :str

      

   .. attribute:: dist_port
      :annotation: :Optional[int]

      

   .. attribute:: nprocs
      :annotation: :Optional[int]

      

   .. method:: dist_url(self)
      :property:


   .. method:: get_rank(self, proc_id)



.. function:: get_dist_options()


.. function:: _init_process_group_fn(proc_id, worker_fn, dist_options, *args)

   wrapper function for worker_fn

   必须定义成可以被pickle的函数。

   1. compute rank
   2. init_process_group
   3. set cuda device if cuda is available
   4. call worker_fn


.. function:: start_distributed_training(worker_fn, dist_options, args = ())

   helper function for distributed training

   1. get number of GPUs
   2. start N process, N = number of GPUs
   3. init_process_group
   4. call worker_fn with args


.. function:: get_available_local_dist_url()

   helper function for single node distributed training

   Get a local dist url like::

       tcp://127.0.0.1:12345



.. function:: init_cpu_process_group(rank = 0, world_size = 1, dist_url = 'tcp://127.0.0.1:12345')

   helper function for testing distributed training

   测试函数，方便在CPU环境下测试分布式操作，比如测试MoCo的shuffle bn。
   不要在生产环境中使用。


.. function:: start_training(worker_fn, args = ())


