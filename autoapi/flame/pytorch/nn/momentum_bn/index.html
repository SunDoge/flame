
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>flame.pytorch.nn.momentum_bn &#8212; flame 0.0.1 documentation</title>
    
  <link href="../../../../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../../../../_static/css/index.f6b7ca918bee2f46fd9abac01cfb07d5.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/graphviz.css" />
    
  <link rel="preload" as="script" href="../../../../../_static/js/index.1e043a052b0af929e4d8.js">

    <script id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
    <script src="../../../../../_static/jquery.js"></script>
    <script src="../../../../../_static/underscore.js"></script>
    <script src="../../../../../_static/doctools.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
    <link rel="next" title="flame.pytorch.processes" href="../../processes/index.html" />
    <link rel="prev" title="flame.pytorch.nn" href="../index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">


    
    <a class="navbar-brand" href="../../../../../index.html">
      <p class="title">flame</p>
    </a>
    

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    
    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../../../../index.html">
  API Reference
 </a>
</li>

        
      </ul>

      <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l2 current active">
  <a class="reference internal" href="../../../index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     flame
    </span>
   </code>
  </a>
  <ul class="current">
   <li class="toctree-l3">
    <a class="reference internal" href="../../../helpers/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.helpers
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3 current active">
    <a class="reference internal" href="../../index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.pytorch
      </span>
     </code>
    </a>
    <ul class="current">
     <li class="toctree-l4">
      <a class="reference internal" href="../../checkpoint_saver/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.checkpoint_saver
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../../experimental/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.experimental
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../../helpers/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.helpers
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../../meters/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.meters
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../../metrics/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.metrics
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4 current active">
      <a class="reference internal" href="../index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.nn
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../../processes/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.processes
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../../utils/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.utils
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../../container/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.container
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../../data_loader_factory/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.data_loader_factory
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../../distributed_training/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.distributed_training
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../../engine/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.engine
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../../experiment/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.experiment
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../../sampler/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.sampler
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../../typing_prelude/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.typing_prelude
        </span>
       </code>
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../../utils/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.utils
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../../archiver/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.archiver
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../../arguments/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.arguments
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../../ckcfg/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.ckcfg
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../../config/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.config
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../../config_parser/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.config_parser
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../../distributed_training/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.distributed_training
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../../logging/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.logging
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../../state/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.state
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../../testing/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.testing
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../../zip_importer/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.zip_importer
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
</ul>
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-contents">
   Module Contents
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classes">
     Classes
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#flame.pytorch.nn.momentum_bn.MomentumBatchNorm2d">
       MomentumBatchNorm2d
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#flame.pytorch.nn.momentum_bn.MomentumBatchNorm2d.momentum_cosine_decay">
         momentum_cosine_decay
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#flame.pytorch.nn.momentum_bn.MomentumBatchNorm2d.forward">
         forward
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#flame.pytorch.nn.momentum_bn.MomentumBatchNorm1d">
       MomentumBatchNorm1d
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#flame.pytorch.nn.momentum_bn.MomentumBatchNorm1d.momentum_cosine_decay">
         momentum_cosine_decay
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#flame.pytorch.nn.momentum_bn.MomentumBatchNorm1d.forward">
         forward
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

</nav>


              
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="module-flame.pytorch.nn.momentum_bn">
<span id="flame-pytorch-nn-momentum-bn"></span><h1><a class="reference internal" href="#module-flame.pytorch.nn.momentum_bn" title="flame.pytorch.nn.momentum_bn"><code class="xref py py-mod docutils literal notranslate"><span class="pre">flame.pytorch.nn.momentum_bn</span></code></a><a class="headerlink" href="#module-flame.pytorch.nn.momentum_bn" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://github.com/zengarden/momentum2-teacher">https://github.com/zengarden/momentum2-teacher</a></p>
<div class="section" id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Permalink to this headline">¶</a></h2>
<div class="section" id="classes">
<h3>Classes<a class="headerlink" href="#classes" title="Permalink to this headline">¶</a></h3>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#flame.pytorch.nn.momentum_bn.MomentumBatchNorm2d" title="flame.pytorch.nn.momentum_bn.MomentumBatchNorm2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MomentumBatchNorm2d</span></code></a></p></td>
<td><p>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#flame.pytorch.nn.momentum_bn.MomentumBatchNorm1d" title="flame.pytorch.nn.momentum_bn.MomentumBatchNorm1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MomentumBatchNorm1d</span></code></a></p></td>
<td><p>Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D</p></td>
</tr>
</tbody>
</table>
<dl class="py class">
<dt id="flame.pytorch.nn.momentum_bn.MomentumBatchNorm2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">flame.pytorch.nn.momentum_bn.</span></code><code class="sig-name descname"><span class="pre">MomentumBatchNorm2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">total_iters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#flame.pytorch.nn.momentum_bn.MomentumBatchNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.BatchNorm2d</span></code></p>
<p>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs
with additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift</a> .</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size). By default, the elements of <span class="math notranslate nohighlight">\(\gamma\)</span> are set
to 1 and the elements of <span class="math notranslate nohighlight">\(\beta\)</span> are set to 0. The standard-deviation is calculated
via the biased estimator, equivalent to <cite>torch.var(input, unbiased=False)</cite>.</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code>
of 0.1.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, H, W)</cite> slices, it’s common terminology to call this Spatial Batch Normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Can be set to <code class="docutils literal notranslate"><span class="pre">None</span></code> for cumulative moving average
(i.e. simple average). Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics, and initializes statistics
buffers <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> as <code class="docutils literal notranslate"><span class="pre">None</span></code>.
When these buffers are <code class="docutils literal notranslate"><span class="pre">None</span></code>, this module always uses batch statistics.
in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="flame.pytorch.nn.momentum_bn.MomentumBatchNorm2d.momentum_cosine_decay">
<code class="sig-name descname"><span class="pre">momentum_cosine_decay</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#flame.pytorch.nn.momentum_bn.MomentumBatchNorm2d.momentum_cosine_decay" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="flame.pytorch.nn.momentum_bn.MomentumBatchNorm2d.forward">
<code class="sig-name descname"><span class="pre">forward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#flame.pytorch.nn.momentum_bn.MomentumBatchNorm2d.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="flame.pytorch.nn.momentum_bn.MomentumBatchNorm1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">flame.pytorch.nn.momentum_bn.</span></code><code class="sig-name descname"><span class="pre">MomentumBatchNorm1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">total_iters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#flame.pytorch.nn.momentum_bn.MomentumBatchNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.BatchNorm1d</span></code></p>
<p>Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D
inputs with optional additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift</a> .</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size). By default, the elements of <span class="math notranslate nohighlight">\(\gamma\)</span> are set
to 1 and the elements of <span class="math notranslate nohighlight">\(\beta\)</span> are set to 0. The standard-deviation is calculated
via the biased estimator, equivalent to <cite>torch.var(input, unbiased=False)</cite>.</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code>
of 0.1.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, L)</cite> slices, it’s common terminology to call this Temporal Batch Normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, L)\)</span> or <span class="math notranslate nohighlight">\(L\)</span> from input of size <span class="math notranslate nohighlight">\((N, L)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Can be set to <code class="docutils literal notranslate"><span class="pre">None</span></code> for cumulative moving average
(i.e. simple average). Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics, and initializes statistics
buffers <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> as <code class="docutils literal notranslate"><span class="pre">None</span></code>.
When these buffers are <code class="docutils literal notranslate"><span class="pre">None</span></code>, this module always uses batch statistics.
in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C)\)</span> or <span class="math notranslate nohighlight">\((N, C, L)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C)\)</span> or <span class="math notranslate nohighlight">\((N, C, L)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="flame.pytorch.nn.momentum_bn.MomentumBatchNorm1d.momentum_cosine_decay">
<code class="sig-name descname"><span class="pre">momentum_cosine_decay</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#flame.pytorch.nn.momentum_bn.MomentumBatchNorm1d.momentum_cosine_decay" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="flame.pytorch.nn.momentum_bn.MomentumBatchNorm1d.forward">
<code class="sig-name descname"><span class="pre">forward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#flame.pytorch.nn.momentum_bn.MomentumBatchNorm1d.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="../index.html" title="previous page"><code class="xref py py-mod docutils literal notranslate"><span class="pre">flame.pytorch.nn</span></code></a>
    <a class='right-next' id="next-link" href="../../processes/index.html" title="next page"><code class="xref py py-mod docutils literal notranslate"><span class="pre">flame.pytorch.processes</span></code></a>

              </div>
              
          </main>
          

      </div>
    </div>

    
  <script src="../../../../../_static/js/index.1e043a052b0af929e4d8.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright 2021, SunDoge.<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.5.4.<br/>
    </p>
  </div>
</footer>
  </body>
</html>