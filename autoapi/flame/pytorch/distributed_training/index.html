
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>flame.pytorch.distributed_training &#8212; flame 0.0.1 documentation</title>
    
  <link href="../../../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../../../_static/css/index.f6b7ca918bee2f46fd9abac01cfb07d5.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/graphviz.css" />
    
  <link rel="preload" as="script" href="../../../../_static/js/index.1e043a052b0af929e4d8.js">

    <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="flame.pytorch.engine" href="../engine/index.html" />
    <link rel="prev" title="flame.pytorch.utils.ranking" href="../utils/ranking/index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">


    
    <a class="navbar-brand" href="../../../../index.html">
      <p class="title">flame</p>
    </a>
    

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    
    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../../../index.html">
  API Reference
 </a>
</li>

        
      </ul>

      <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l2 current active">
  <a class="reference internal" href="../../index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     flame
    </span>
   </code>
  </a>
  <ul class="current">
   <li class="toctree-l3 current active">
    <a class="reference internal" href="../index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.pytorch
      </span>
     </code>
    </a>
    <ul class="current">
     <li class="toctree-l4">
      <a class="reference internal" href="../utils/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.utils
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4 current active">
      <a class="current reference internal" href="#">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.distributed_training
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../engine/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.engine
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../process/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.process
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../typing_prelude/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.typing_prelude
        </span>
       </code>
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../utils/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.utils
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../archiver/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.archiver
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../argument/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.argument
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../auto_builder/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.auto_builder
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../ckcfg/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.ckcfg
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../config/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.config
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../logging/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.logging
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
</ul>
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-contents">
   Module Contents
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classes">
     Classes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#functions">
     Functions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#flame.pytorch.distributed_training._logger">
       _logger
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#flame.pytorch.distributed_training.DistOptions">
       DistOptions
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#flame.pytorch.distributed_training.DistOptions.rank_start">
         rank_start
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#flame.pytorch.distributed_training.DistOptions.world_size">
         world_size
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#flame.pytorch.distributed_training.DistOptions.dist_backend">
         dist_backend
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#flame.pytorch.distributed_training.DistOptions.dist_url">
         dist_url
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#flame.pytorch.distributed_training._init_process_group_fn">
       _init_process_group_fn
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#flame.pytorch.distributed_training.start_distributed_training">
       start_distributed_training
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#flame.pytorch.distributed_training.get_available_local_dist_url">
       get_available_local_dist_url
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#flame.pytorch.distributed_training.init_cpu_process_group">
       init_cpu_process_group
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

</nav>


              
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="module-flame.pytorch.distributed_training">
<span id="flame-pytorch-distributed-training"></span><h1><a class="reference internal" href="#module-flame.pytorch.distributed_training" title="flame.pytorch.distributed_training"><code class="xref py py-mod docutils literal notranslate"><span class="pre">flame.pytorch.distributed_training</span></code></a><a class="headerlink" href="#module-flame.pytorch.distributed_training" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Permalink to this headline">¶</a></h2>
<div class="section" id="classes">
<h3>Classes<a class="headerlink" href="#classes" title="Permalink to this headline">¶</a></h3>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#flame.pytorch.distributed_training.DistOptions" title="flame.pytorch.distributed_training.DistOptions"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DistOptions</span></code></a></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="functions">
<h3>Functions<a class="headerlink" href="#functions" title="Permalink to this headline">¶</a></h3>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#flame.pytorch.distributed_training._init_process_group_fn" title="flame.pytorch.distributed_training._init_process_group_fn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_init_process_group_fn</span></code></a>(device_id: int, worker_fn: Callable, dist_options: flame.pytorch.distributed_training.DistOptions, *args)</p></td>
<td><p>wrapper function for worker_fn</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#flame.pytorch.distributed_training.start_distributed_training" title="flame.pytorch.distributed_training.start_distributed_training"><code class="xref py py-obj docutils literal notranslate"><span class="pre">start_distributed_training</span></code></a>(worker_fn: Callable, args: tuple = (), rank_start: int = 0, world_size: int = 1, dist_backend: str = 'NCCL', dist_url: str = 'tcp://127.0.0.1:12345')</p></td>
<td><p>helper function for distributed training</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#flame.pytorch.distributed_training.get_available_local_dist_url" title="flame.pytorch.distributed_training.get_available_local_dist_url"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_available_local_dist_url</span></code></a>() → str</p></td>
<td><p>helper function for single node distributed training</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#flame.pytorch.distributed_training.init_cpu_process_group" title="flame.pytorch.distributed_training.init_cpu_process_group"><code class="xref py py-obj docutils literal notranslate"><span class="pre">init_cpu_process_group</span></code></a>(rank: int = 0, world_size: int = 1, dist_url: str = 'tcp://127.0.0.1:12345')</p></td>
<td><p>helper function for testing distributed training</p></td>
</tr>
</tbody>
</table>
<dl class="py data">
<dt id="flame.pytorch.distributed_training._logger">
<code class="sig-prename descclassname"><span class="pre">flame.pytorch.distributed_training.</span></code><code class="sig-name descname"><span class="pre">_logger</span></code><a class="headerlink" href="#flame.pytorch.distributed_training._logger" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py class">
<dt id="flame.pytorch.distributed_training.DistOptions">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">flame.pytorch.distributed_training.</span></code><code class="sig-name descname"><span class="pre">DistOptions</span></code><a class="headerlink" href="#flame.pytorch.distributed_training.DistOptions" title="Permalink to this definition">¶</a></dt>
<dd><dl class="py attribute">
<dt id="flame.pytorch.distributed_training.DistOptions.rank_start">
<code class="sig-name descname"><span class="pre">rank_start</span></code><em class="property"> <span class="pre">:int</span> <span class="pre">=</span> <span class="pre">0</span></em><a class="headerlink" href="#flame.pytorch.distributed_training.DistOptions.rank_start" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="flame.pytorch.distributed_training.DistOptions.world_size">
<code class="sig-name descname"><span class="pre">world_size</span></code><em class="property"> <span class="pre">:int</span> <span class="pre">=</span> <span class="pre">1</span></em><a class="headerlink" href="#flame.pytorch.distributed_training.DistOptions.world_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="flame.pytorch.distributed_training.DistOptions.dist_backend">
<code class="sig-name descname"><span class="pre">dist_backend</span></code><em class="property"> <span class="pre">:str</span> <span class="pre">=</span> <span class="pre">NCCL</span></em><a class="headerlink" href="#flame.pytorch.distributed_training.DistOptions.dist_backend" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="flame.pytorch.distributed_training.DistOptions.dist_url">
<code class="sig-name descname"><span class="pre">dist_url</span></code><em class="property"> <span class="pre">:str</span> <span class="pre">=</span> <span class="pre">tcp://127.0.0.1:12345</span></em><a class="headerlink" href="#flame.pytorch.distributed_training.DistOptions.dist_url" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="flame.pytorch.distributed_training._init_process_group_fn">
<code class="sig-prename descclassname"><span class="pre">flame.pytorch.distributed_training.</span></code><code class="sig-name descname"><span class="pre">_init_process_group_fn</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker_fn</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Callable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist_options</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#flame.pytorch.distributed_training.DistOptions" title="flame.pytorch.distributed_training.DistOptions"><span class="pre">flame.pytorch.distributed_training.DistOptions</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#flame.pytorch.distributed_training._init_process_group_fn" title="Permalink to this definition">¶</a></dt>
<dd><p>wrapper function for worker_fn</p>
<p>必须定义成可以被pickle的函数。</p>
<ol class="arabic simple">
<li><p>compute rank</p></li>
<li><p>init_process_group</p></li>
<li><p>set cuda device if cuda is available</p></li>
<li><p>call worker_fn</p></li>
</ol>
</dd></dl>

<dl class="py function">
<dt id="flame.pytorch.distributed_training.start_distributed_training">
<code class="sig-prename descclassname"><span class="pre">flame.pytorch.distributed_training.</span></code><code class="sig-name descname"><span class="pre">start_distributed_training</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">worker_fn</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Callable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">tuple</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_start</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">world_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist_backend</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'NCCL'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist_url</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'tcp://127.0.0.1:12345'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#flame.pytorch.distributed_training.start_distributed_training" title="Permalink to this definition">¶</a></dt>
<dd><p>helper function for distributed training</p>
<ol class="arabic simple">
<li><p>get number of GPUs</p></li>
<li><p>start N process, N = number of GPUs</p></li>
<li><p>init_process_group</p></li>
<li><p>call worker_fn with <a href="#id1"><span class="problematic" id="id2">*</span></a>args</p></li>
</ol>
</dd></dl>

<dl class="py function">
<dt id="flame.pytorch.distributed_training.get_available_local_dist_url">
<code class="sig-prename descclassname"><span class="pre">flame.pytorch.distributed_training.</span></code><code class="sig-name descname"><span class="pre">get_available_local_dist_url</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">str</span><a class="headerlink" href="#flame.pytorch.distributed_training.get_available_local_dist_url" title="Permalink to this definition">¶</a></dt>
<dd><p>helper function for single node distributed training</p>
<p>Get a local dist url like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tcp</span><span class="p">:</span><span class="o">//</span><span class="mf">127.0</span><span class="o">.</span><span class="mf">0.1</span><span class="p">:</span><span class="mi">12345</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="flame.pytorch.distributed_training.init_cpu_process_group">
<code class="sig-prename descclassname"><span class="pre">flame.pytorch.distributed_training.</span></code><code class="sig-name descname"><span class="pre">init_cpu_process_group</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rank</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">world_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist_url</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'tcp://127.0.0.1:12345'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#flame.pytorch.distributed_training.init_cpu_process_group" title="Permalink to this definition">¶</a></dt>
<dd><p>helper function for testing distributed training</p>
<p>测试函数，方便在CPU环境下测试分布式操作，比如测试MoCo的shuffle bn。
不要在生产环境中使用。</p>
</dd></dl>

</div>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="../utils/ranking/index.html" title="previous page"><code class="xref py py-mod docutils literal notranslate"><span class="pre">flame.pytorch.utils.ranking</span></code></a>
    <a class='right-next' id="next-link" href="../engine/index.html" title="next page"><code class="xref py py-mod docutils literal notranslate"><span class="pre">flame.pytorch.engine</span></code></a>

              </div>
              
          </main>
          

      </div>
    </div>

    
  <script src="../../../../_static/js/index.1e043a052b0af929e4d8.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright 2021, SunDoge.<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.5.3.<br/>
    </p>
  </div>
</footer>
  </body>
</html>