
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>flame.pytorch.sampler &#8212; flame 0.0.1 documentation</title>
    
  <link href="../../../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../../../_static/css/index.f6b7ca918bee2f46fd9abac01cfb07d5.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/graphviz.css" />
    
  <link rel="preload" as="script" href="../../../../_static/js/index.1e043a052b0af929e4d8.js">

    <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="flame.pytorch.typing_prelude" href="../typing_prelude/index.html" />
    <link rel="prev" title="flame.pytorch.experiment" href="../experiment/index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">


    
    <a class="navbar-brand" href="../../../../index.html">
      <p class="title">flame</p>
    </a>
    

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    
    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../../../index.html">
  API Reference
 </a>
</li>

        
      </ul>

      <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l2 current active">
  <a class="reference internal" href="../../index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     flame
    </span>
   </code>
  </a>
  <ul class="current">
   <li class="toctree-l3">
    <a class="reference internal" href="../../helpers/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.helpers
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3 current active">
    <a class="reference internal" href="../index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.pytorch
      </span>
     </code>
    </a>
    <ul class="current">
     <li class="toctree-l4">
      <a class="reference internal" href="../checkpoint_saver/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.checkpoint_saver
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../experimental/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.experimental
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../helpers/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.helpers
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../meters/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.meters
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../metrics/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.metrics
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../nn/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.nn
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../processes/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.processes
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../utils/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.utils
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../container/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.container
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../data_loader_factory/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.data_loader_factory
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../distributed_training/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.distributed_training
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../engine/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.engine
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../experiment/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.experiment
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4 current active">
      <a class="current reference internal" href="#">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.sampler
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l4">
      <a class="reference internal" href="../typing_prelude/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         flame.pytorch.typing_prelude
        </span>
       </code>
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../utils/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.utils
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../archiver/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.archiver
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../arguments/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.arguments
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../ckcfg/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.ckcfg
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../config/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.config
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../config_parser/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.config_parser
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../distributed_training/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.distributed_training
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../logging/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.logging
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../state/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.state
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../testing/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.testing
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="../../zip_importer/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       flame.zip_importer
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
</ul>
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-contents">
   Module Contents
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classes">
     Classes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#functions">
     Functions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#flame.pytorch.sampler.UniformDistributedSampler">
       UniformDistributedSampler
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#flame.pytorch.sampler.UniformDistributedSampler.__iter__">
         __iter__
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#flame.pytorch.sampler.remove_padding">
       remove_padding
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

</nav>


              
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="module-flame.pytorch.sampler">
<span id="flame-pytorch-sampler"></span><h1><a class="reference internal" href="#module-flame.pytorch.sampler" title="flame.pytorch.sampler"><code class="xref py py-mod docutils literal notranslate"><span class="pre">flame.pytorch.sampler</span></code></a><a class="headerlink" href="#module-flame.pytorch.sampler" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Permalink to this headline">¶</a></h2>
<div class="section" id="classes">
<h3>Classes<a class="headerlink" href="#classes" title="Permalink to this headline">¶</a></h3>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#flame.pytorch.sampler.UniformDistributedSampler" title="flame.pytorch.sampler.UniformDistributedSampler"><code class="xref py py-obj docutils literal notranslate"><span class="pre">UniformDistributedSampler</span></code></a></p></td>
<td><p>Sampler that restricts data loading to a subset of the dataset.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="functions">
<h3>Functions<a class="headerlink" href="#functions" title="Permalink to this headline">¶</a></h3>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#flame.pytorch.sampler.remove_padding" title="flame.pytorch.sampler.remove_padding"><code class="xref py py-obj docutils literal notranslate"><span class="pre">remove_padding</span></code></a>(x, indices)</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py class">
<dt id="flame.pytorch.sampler.UniformDistributedSampler">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">flame.pytorch.sampler.</span></code><code class="sig-name descname"><span class="pre">UniformDistributedSampler</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_replicas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_last</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#flame.pytorch.sampler.UniformDistributedSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.distributed.DistributedSampler</span></code></p>
<p>Sampler that restricts data loading to a subset of the dataset.</p>
<p>It is especially useful in conjunction with
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.parallel.DistributedDataParallel</span></code>. In such a case, each
process can pass a <code class="xref py py-class docutils literal notranslate"><span class="pre">DistributedSampler</span></code> instance as a
<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> sampler, and load a subset of the
original dataset that is exclusive to it.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Dataset is assumed to be of constant size.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> – Dataset used for sampling.</p></li>
<li><p><strong>num_replicas</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of processes participating in
distributed training. By default, <code class="xref py py-attr docutils literal notranslate"><span class="pre">world_size</span></code> is retrieved from the
current distributed group.</p></li>
<li><p><strong>rank</strong> (<em>int</em><em>, </em><em>optional</em>) – Rank of the current process within <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_replicas</span></code>.
By default, <code class="xref py py-attr docutils literal notranslate"><span class="pre">rank</span></code> is retrieved from the current distributed
group.</p></li>
<li><p><strong>shuffle</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> (default), sampler will shuffle the
indices.</p></li>
<li><p><strong>seed</strong> (<em>int</em><em>, </em><em>optional</em>) – random seed used to shuffle the sampler if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">shuffle=True</span></code>. This number should be identical across all
processes in the distributed group. Default: <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p><strong>drop_last</strong> (<em>bool</em><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the sampler will drop the
tail of the data to make it evenly divisible across the number of
replicas. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the sampler will add extra indices to make
the data evenly divisible across the replicas. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>In distributed mode, calling the <code class="xref py py-meth docutils literal notranslate"><span class="pre">set_epoch()</span></code> method at
the beginning of each epoch <strong>before</strong> creating the <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> iterator
is necessary to make shuffling work properly across multiple epochs. Otherwise,
the same ordering will be always used.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="k">if</span> <span class="n">is_distributed</span> <span class="k">else</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="p">(</span><span class="n">sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span>
<span class="gp">... </span>                    <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_epoch</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">if</span> <span class="n">is_distributed</span><span class="p">:</span>
<span class="gp">... </span>        <span class="n">sampler</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">train</span><span class="p">(</span><span class="n">loader</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="flame.pytorch.sampler.UniformDistributedSampler.__iter__">
<code class="sig-name descname"><span class="pre">__iter__</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#flame.pytorch.sampler.UniformDistributedSampler.__iter__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Iterable[int]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="flame.pytorch.sampler.remove_padding">
<code class="sig-prename descclassname"><span class="pre">flame.pytorch.sampler.</span></code><code class="sig-name descname"><span class="pre">remove_padding</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#flame.pytorch.sampler.remove_padding" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – </p></li>
<li><p><strong>indices</strong> (<em>torch.Tensor</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="../experiment/index.html" title="previous page"><code class="xref py py-mod docutils literal notranslate"><span class="pre">flame.pytorch.experiment</span></code></a>
    <a class='right-next' id="next-link" href="../typing_prelude/index.html" title="next page"><code class="xref py py-mod docutils literal notranslate"><span class="pre">flame.pytorch.typing_prelude</span></code></a>

              </div>
              
          </main>
          

      </div>
    </div>

    
  <script src="../../../../_static/js/index.1e043a052b0af929e4d8.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright 2021, SunDoge.<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.5.4.<br/>
    </p>
  </div>
</footer>
  </body>
</html>